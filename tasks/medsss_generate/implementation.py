def medsss_generate(user_message: str = "How to stop a cough?") -> dict:
    """
    Given a user message, generate a response using the MedSSS_Policy model.

    Args:
        user_message: The user message.

    Returns:
        dict with the following structure:
        {
          'response': str  # The response generated by the model.
        }
    """

    # Load the MedSSS_Policy model
    # See: https://huggingface.co/pixas/MedSSS_Policy
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer

    model = AutoModelForCausalLM.from_pretrained(
        "pixas/MedSSS_Policy",
        torch_dtype=torch.float16,  # Use float16 for numerical stability
        device_map="auto"
    )
    model.eval()  # Set to evaluation mode
    tokenizer = AutoTokenizer.from_pretrained("pixas/MedSSS_Policy")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    messages = [{"role": "user", "content": user_message}]
    inputs = tokenizer(
        tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        ),
        return_tensors="pt",
    ).to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=2048,
            do_sample=False  # Use greedy decoding for stability
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}
